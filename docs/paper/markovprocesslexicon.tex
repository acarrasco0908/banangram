\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb, amsmath, bm}
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{array}
\usepackage{filecontents, natbib}% making in-document bibliography
\usepackage{mathtools}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% User defined macros
% partial derivative
\newcommand{\partder}[2]{\frac{\partial#1}{\partial#2}}

\title{Markov Process Lexicon}
\begin{document}
\maketitle

\section{Introduction}
We are interested in modeling the generation of a random lexicon as a Markov process. We will need to check
\begin{enumerate}
\item the columns sum to 1 (so the transition to the next state is a probability),
\item the largest eigenvalue is equal to 1,
\item the lexicon generated is finite (finite words are generated).
\end{enumerate}


\section{Word Generation}

In the simplest formation of markov process language generation, we can imagine creating words randomly until, with a random chance $\gamma$ of ending the word. To avoid single letter words we need to start the chain with two letters (we will solve this problem differently in later sections. The transition matrix for this (left) Markov process may be expressed
\begin{equation}
P_{ij}=\left[\begin{array}{c c c c c c}
(1-\gamma)/N & \cdots & (1-\gamma)/N& 0\\
\vdots& & \vdots & \vdots\\
 (1-\gamma)/N & \cdots & (1-\gamma)/N & 0\\
\gamma & \cdots &\gamma & 1
\end{array}
\right].
\end{equation}

\begin{align}
\mathbf{v_{NW}}& = \left[\begin{array}{c}
(1-\gamma)/N \\ \vdots \\ (1-\gamma)/N \\ \gamma
\end{array}\right] & 
\mathbf{v_W}& = \left[\begin{array}{c}
0 \\ \vdots \\ 0 \\ 1
\end{array}\right] & 
\end{align}

\begin{align}
P_{ij}\mathbf{v_{NW}} &= \left[\begin{array}{c}
N((1-\gamma)/N)^2\\\vdots\\N((1-\gamma)/N)^2\\N\gamma(1-\gamma)/N + \gamma
\end{array}\right] = \left[\begin{array}{c}
(1-\gamma)(1-\gamma)/N\\\vdots\\(1-\gamma)(1-\gamma)/N\\ \gamma(2-\gamma)
\end{array}\right]\\ 
&=(1-\gamma)\mathbf{v_{NW}} + \gamma\mathbf{v_W}
\end{align}

\begin{equation}
P_{ij}\mathbf{v_W} = \left[\begin{array}{c}
0\\\vdots\\0\\1
\end{array}\right] = \mathbf{v_W}
\end{equation}

Write
\begin{equation}
P = \left[\begin{array}{c c}
Q & 0\\
R & I
\end{array}\right],
\end{equation}
Where $Q$ represents the transient-to-transient transitions, $R$ the transient-to-absorbed transitions, and $I$ is the identity matrix with rank corresponding to the number of absorbing states. 
\begin{equation}
Q=\left[\begin{array}{c c c c c c}
(1-\gamma)/N & \cdots & (1-\gamma)/N\\
\vdots& & \vdots \\
 (1-\gamma)/N & \cdots & (1-\gamma)/N
 \end{array}\right]
 \end{equation}

The matrix
\begin{equation}
N = \sum_{k=0}Q^k = (I - Q)^{-1}
\end{equation}
represents the number of times a transient state is expected to be hit on the way to an absorbing state starting from a specific place. If we substitute our $Q$ matrix, we find
\begin{equation}
N = I + \frac{1}{\gamma}Q
\end{equation}

Note:
\begin{equation}
NR = (I + \frac{1}{\gamma}Q)\gamma\mathbf{1} = \gamma \mathbf{1} + Q\mathbf{1} = (\gamma + (1-\gamma))\mathbf{1} = \mathbf{1}
\end{equation}

\begin{equation}
N\mathbf{1} = (I + \frac{1}{\gamma}Q)\mathbf{1} = \mathbf{1} + \frac{1}{\gamma}Q\mathbf{1} = (1 + \frac{1-\gamma}{\gamma})\mathbf{1} = \frac{1}{\gamma}\mathbf{1}
\end{equation}


\begin{equation}
<\ell> = 1 + 1 + N\frac{(1-\gamma)}{N\gamma} = \frac{\gamma +1}{\gamma}
\end{equation}



\section{Lexicon Branch Generation}
For the process, we imagine stringing characters together until, at some random point, the string is considered a word. At another random point, it's decided that no more words can be made using the last word as a starting point. There are thus three qualitatively different states for the Markov process (and degeneracies corresponding to the number of allowable characters). 

The possible states from the first letter come from adding a character that does not finish a word, adding a character that finishes a word, and the bottom node (corresponding to a finished word that has no suffixes). For $N$ characters, the transition matrix $P$ for this process has a shape of $2N+1\times2N+1$ ($N$ characters that don't finish words, $N$ characters that could finish words, and the bottom node). If we imagine each character as equally likely and completable words being completed with probability $\alpha$, then the transition matrix can be expressed as
\begin{equation}
P_{ij}=\left[\begin{array}{c c c c c c}
1/2N&\cdots & (1-\alpha)/2N & \cdots & 0\\
 \vdots & & \vdots & & \vdots\\
1/2N & \cdots & (1-\alpha)/2N & \cdots & 0\\
1/2N & \cdots & (1-\alpha)/2N & \cdots & 0\\
\vdots & & \vdots & & \vdots\\
1/2N & \cdots & (1-\alpha)/2N & \cdots & 0\\
0 & \cdots & \alpha & \cdots & 1
\end{array}
\right].
\end{equation}

We've got to unpack this a little. The first $N$ rows correspond to the $N$ characters that don't end words. The next $N$ rows are the $N$ characters that end words. Characters that do and do not end words may transition to each other (e.g., $\circ$en\=d might transition to $\circ$en\=di on its way to $\circ$en\=din\=g\=s). The latter selects the next character with equal probability ($1/2N$), but the former must retain the possibility of actually ending the branch (as no word can be constructed from $\circ$en\=din\=g\=s$\bullet$). We denote the probability of ending a branch as $\alpha$. This $\alpha$ factor is removed uniformly from the transition probabilities away from a character that end words, hence $(1-\alpha)/2N$. Finally, the last row corresponds to the bottom node state, that cannot transition to anywhere.

The columns can similarly be partitioned into those three categories, where we define the following vectors
\begin{align}
\mathbf{v_{NW}}& = \left[\begin{array}{c}
 1/2N \\ \vdots \\ 1/2N \\ 1/2N \\ \vdots \\ 1/2N \\ 0
\end{array}\right] & 
\mathbf{v_W}& = \left[\begin{array}{c}
(1-\alpha)/2N \\ \vdots \\ (1-\alpha)/2N \\ (1-\alpha)/2N \\ \vdots \\ (1-\alpha)/2N \\ \alpha
\end{array}\right] & 
\mathbf{v_B}& = \left[\begin{array}{c}
0 \\ \vdots \\ 0 \\ 0 \\ \vdots \\ 0 \\ 1
\end{array}\right] & 
\end{align}
Clearly there are many linear dependencies between the columns (as there are $N$ each of $\mathbf{v_{NW}}$ and $\mathbf{v_W}$). An additional linear dependence that is of note is
\begin{equation}
\mathbf{v_W} = \alpha\mathbf{v_B} + (1-\alpha)\mathbf{v_{NW}}.
\end{equation}
The existence of this linear relationship and the number of degeneracies in the matrix $P$ suggests that there should only be two nonzero eigenvalues of $P$.

Before jumping to the eigendecomposition, let's explore some of the properties of $P$. First, the inner spaces:
\begin{align}
\mathbf{v_B}\cdot\mathbf{v_B} &= 1\\
\mathbf{v_{NW}}\cdot\mathbf{v_{NW}} &= 1/2N\\
\mathbf{v_W}\cdot\mathbf{v_W} &= (1 - 2\alpha + (1+2N)\alpha^2)/2N\\
\mathbf{v_B}\cdot\mathbf{v_{NW}} &= 0\\
\mathbf{v_B}\cdot\mathbf{v_W} &= \alpha\\
\mathbf{v_{NW}}\cdot\mathbf{v_W} &= (1-\alpha)/2N.\\
\end{align}
Now, evaluate how each of these vectors is acted upon by $P$.
\begin{align}
P_{ij}\mathbf{v_{NW}} &= \left[\begin{array}{c}
N(1/2N)^2 + N(1/2N)((1-\alpha)/2N)\\\vdots\\N(1/2N)^2 + N(1/2N)((1-\alpha)/2N)\\N(1/2N)\alpha
\end{array}\right] = \left[\begin{array}{c}
(2-\alpha)/4N\\\vdots\\(2-\alpha)/4N\\\alpha/2
\end{array}\right]\\ 
&= \frac{2-\alpha}{2}\mathbf{v_{NW}} + \frac{\alpha}{2}\mathbf{v_B}
\end{align}
\begin{align}
P_{ij}\mathbf{v_{W}} &= \left[\begin{array}{c}
N(1/2N)((1-\alpha)/2N) + N((1-\alpha)/2N)^2\\\vdots\\N(1/2N)((1-\alpha)/2N) + N((1-\alpha)/2N)^2\\N\alpha(1-\alpha)/2N + \alpha
\end{array}\right] = \left[\begin{array}{c}
(1-\alpha)(2-\alpha)/4N\\\vdots\\(1-\alpha)(2-\alpha)/4N\\ \alpha(3-\alpha)/2
\end{array}\right]\\ 
&=\frac{2-\alpha}{2}\mathbf{v_W} + \frac{\alpha}{2}\mathbf{v_B}\label{eq:Pvw}
\end{align}
\begin{equation}\label{eq:Pvb}
P_{ij}\mathbf{v_B} = \left[\begin{array}{c}
0\\\vdots\\0\\1
\end{array}\right] = \mathbf{v_B}
\end{equation}
Observe that the bottom node state transitions only to itself, and is therefore our first eigenvector, and has an eigenvalue of one.

Now we need to find the second unique eigenvalue, and check that it isn't greater than 1. Try
\begin{equation}
\mathbf{v_2} = \mathbf{v_B} - \mathbf{v_{NW}} = \left[\begin{array}{c}
-1/2N \\ \vdots \\ -1/2N \\ -1/2N \\ \vdots \\ -1/2N \\ 1
\end{array}\right],
\end{equation}
with norm
\begin{equation}
\mathbf{v_2}\cdot\mathbf{v_2} = 2N\frac{1}{4N^2} + 1 = \frac{1 + 2N}{2N}.
\end{equation}
When acted upon by $P$, we see
\begin{align}
P_{ij}\mathbf{v_2} &= \left[\begin{array}{c}
N(-1/2N)(1/2N) + N(-1/2N)((1-\alpha)/2N)\\
\vdots\\
N(-1/2N)(1/2N) + N(-1/2N)((1-\alpha)/2N) \\
N(-1/2N)\alpha + 1
\end{array}\right] \\
&=\left[\begin{array}{c}
((2-\alpha)/2)(-1/2N)\\
\vdots\\
((2-\alpha)/2)(-1/2N) \\
(2-\alpha)/2
\end{array}\right]= \frac{2-\alpha}{2}\left[\begin{array}{c}
-1/2N\\\vdots\\-1/2N\\1
\end{array}\right]=\frac{2-\alpha}{2}\mathbf{v_2}
\end{align}

Alternatively,
\begin{align}
P\mathbf{v_2} &= P(\mathbf{v_B} - \mathbf{v_{NW}}) = \mathbf{v_B} - (\frac{2-\alpha}{2}\mathbf{v_{NW}} + \frac{\alpha}{2}\mathbf{v_B})\\
&=\frac{1-\alpha}{2}(\mathbf{v_B} - \mathbf{v_{NW}}) = 1-\frac{\alpha}{2}\mathbf{v_2}
\end{align}


Now we may write our starting state $\mathbf{v_{NW}}$ as 
\begin{equation}
\mathbf{v_{NW}} = \mathbf{v_B} - \mathbf{v_2}
\end{equation}

Now, since
\begin{equation}
P\mathbf{v_{NW}} = \mathbf{v_B} -  \frac{2-\alpha}{2}\mathbf{v_2},
\end{equation}
repeated application of the transition matrix yields
\begin{equation}
P^n\mathbf{v_{NW}} = \mathbf{v_B} -  \left(\frac{2-\alpha}{2}\right)^n\mathbf{v_2}.
\end{equation}
As long as $0<\alpha\le1$, we know that $(2-\alpha)/2<1$, implying that
\begin{equation}
\lim_{n\to\infty}P^n\mathbf{v_{NW}} = \mathbf{v_B},
\end{equation}
which means that all our branches must eventually end and so we may expect finite words.


\section{Fundamental Matrix}
\begin{equation}
N = \sum_{k=0}^\infty Q^k = (I_t - Q)^{-1}
\end{equation}

Now, our $Q$ is
\begin{equation}
Q = \left[\begin{array}{c c c}
1/2N& \cdots& 1/2N\\
\vdots & &\vdots\\
1/2N& \cdots& 1/2N\\
(1-\alpha)/2N& \cdots& (1-\alpha)/2N\\
\vdots & &\vdots\\
(1-\alpha)/2N& \cdots& (1-\alpha)/2N
\end{array}\right]
\end{equation}

\begin{equation}
Q^* = \left[\begin{array}{c c c}
1/N\alpha& \cdots& 1/N\alpha\\
\vdots & &\vdots\\
1/N\alpha& \cdots& 1/N\alpha\\
(1-\alpha)/N\alpha& \cdots& (1-\alpha)/N\alpha\\
\vdots & &\vdots\\
(1-\alpha)/N\alpha& \cdots& (1-\alpha)/N\alpha
\end{array}\right]
\end{equation}

\begin{equation}
N = (I - Q)^{-1} = I + Q^*
\end{equation}

The expected length of a branch (the expected number of steps before hitting the absorbing bottom node state) is $t_\textrm{branch}=1+2N(1/N\alpha)=1+2/\alpha$. As the probability of ending the branch decreases, this increases.

The number of words in a branch corresponds to the number of times the chain visits a word-ending character. The entries $N_{ij}$ are the expected number of times that transient state $j$ is visited before being absorbed, given that the chain started in state $i$. We will always be starting in a nonword character state, and we don't care which word-ending character we hit. Summing along one of the rows corresponding to starting at a non-word ending character, we see that the expected number of words per chain is $N/N\alpha = 1/\alpha$.

To get the expected word length, we need to compute the number of times we visit one of the characters that end words (which are transient states). The probability of visiting 
\begin{equation}
H = (N-I_t)\textrm{diag}^{-1}(N)
\end{equation}

\section{Word Branch Generation, tunable word probability}

\begin{equation}
P_{ij}=\left[\begin{array}{c c c c c c}
(1-\beta)/N&\cdots & (1-\alpha)(1-\beta)/N & \cdots & 0\\
 \vdots & & \vdots & & \vdots\\
(1-\beta)/N & \cdots & (1-\alpha)(1-\beta)/N & \cdots & 0\\
\beta/N & \cdots & (1-\alpha)\beta/N & \cdots & 0\\
\vdots & & \vdots & & \vdots\\
\beta/N & \cdots & (1-\alpha)\beta/N & \cdots & 0\\
0 & \cdots & \alpha & \cdots & 1
\end{array}
\right].
\end{equation}

\begin{align}
\mathbf{v_{NW}}& = \left[\begin{array}{c}
(1-\beta)/N \\ \vdots \\ (1-\beta)/N \\ \beta/N \\ \vdots \\ \beta/N \\ 0
\end{array}\right] & 
\mathbf{v_W}& = \left[\begin{array}{c}
 (1-\alpha)(1-\beta)/N \\ \vdots \\ (1-\alpha)(1-\beta)/N \\ (1-\alpha)\beta/N \\ \vdots \\ (1-\alpha)\beta/N \\ \alpha
\end{array}\right] & 
\mathbf{v_B}& = \left[\begin{array}{c}
0 \\ \vdots \\ 0 \\ 0 \\ \vdots \\ 0 \\ 1
\end{array}\right] & 
\end{align}

\begin{equation}
\mathbf{v_{NW}}\cdot\mathbf{v_W} = \frac{1-\alpha}{N^2}(N(1-\beta)^2 + N\beta^2) = \frac{1-\alpha}{N}(1 - 2\beta + 2\beta^2)
\end{equation} 

\begin{equation}
Q=\left[\begin{array}{c c c c c c}
(1-\beta)/N&\cdots & (1-\alpha)(1-\beta)/N & \cdots\\
 \vdots & & \vdots & & \vdots\\
(1-\beta)/N & \cdots & (1-\alpha)(1-\beta)/N & \cdots\\
\beta/N & \cdots & (1-\alpha)\beta/N & \cdots \\
\vdots & & \vdots & & \vdots\\
\beta/N & \cdots & (1-\alpha)\beta/N & \cdots\\
\end{array}
\right].
\end{equation}

\begin{align}
Q^2 &= (1-\alpha\beta)Q\\
\implies Q^k &= (1-\alpha\beta)^{k-1}Q
\end{align}
Thus
\begin{equation}
N = \sum_{k=0}^\infty Q^k = I + \frac{1}{\alpha\beta}Q.
\end{equation}

The expected length of a chain starting from the not-word state
\begin{align}
(N\mathbf{1})_{NW} &= (I + \frac{1}{\alpha\beta}Q)\mathbf{1} = \mathbf{1} + \frac{1}{\alpha\beta}Q\mathbf{1} \\ &= \left(1 + \frac{1}{\alpha\beta}\left(N\frac{1-\beta}{N} + N\frac{(1-\alpha)(1-\beta)}{N}\right)\right)\mathbf{1} \\
&=\left(1 + \frac{1}{\alpha\beta}(1-\beta)(2-\alpha))\right)\mathbf{1}
\end{align}

\begin{align}
\sum_{k=0}^\infty k Q^k &= \sum_{k=1}^\infty k(1-\alpha\beta)^{k-1}\frac{\beta}{N}\\
&=\frac{\beta}{N(1-\alpha\beta)}\sum_{k=1}^\infty k(1-\alpha\beta)^k \\
&=\frac{\beta}{N(1-\alpha\beta)}\frac{(1-\alpha\beta)}{(1-(1-\alpha\beta))^2}\\ & = \frac{\beta}{N\alpha^2\beta^2}
\end{align}


\appendix
\section{Errata}
We are interested in modeling the generation of a random lexicon as a Markov process. For the process, we imagine stringing characters together until, at some random point, the string is considered a word. There are thus four qualitatively different states for the Markov process (and degeneracies corresponding to the number of allowable characters). We will need to check
\begin{enumerate}
\item the columns sum to 1 (so the transition to the next state is a probability),
\item the largest eigenvalue is equal to 1,
\item the lexicon generated is finite (finite words are generated).
\end{enumerate}
The possible states are the top node (before adding the first letter), adding a character that does not finish a word, adding a character that finishes a word, and the bottom node (corresponding to a finished word that has no suffixes). For $N$ characters, the transition matrix $P$ for this process has a shape of $2(N+1)\times2(N+1)$ (2 edge nodes, $N$ characters that don't finish words, $N$ characters that could finish words). If we imagine each character as equally likely and completable words being completed with probability $\alpha$, then the transition matrix can be expressed as
\begin{equation}
P_{ij}=\left[\begin{array}{c c c c c c}
0 & 0 & 0 & 0 & 0 & 0 \\
1/N & 1/2N&\cdots & (1-\alpha)/2N & \cdots & 0\\
\vdots & \vdots & & \vdots & & \vdots\\
1/N & 1/2N & \cdots & (1-\alpha)/2N & \cdots & 0\\
0 & 1/2N & \cdots & (1-\alpha)/2N & \cdots & 0\\
\vdots & \vdots & & \vdots & & \vdots\\
0 & 1/2N & \cdots & (1-\alpha)/2N & \cdots & 0\\
0 & 0 & \cdots & \alpha & \cdots & 1
\end{array}
\right].
\end{equation}

We've got to unpack this a little. The first row represents the top node state - no state should be able to return to the top node, so it is filled with zeros. The next $N$ rows correspond to the $N$ characters that don't end words. The top node leads only to these to exclude words that are only one character long, but it can lead to any of the characters with equal probability, hence the $1/N$ populating that segment of the first column. The next $N$ rows are the $N$ characters that end words. Characters that do and do not end words may transition to each other (e.g., $\circ$en\=d might transition to $\circ$en\=di on its way to $\circ$en\=din\=g\=s). The latter selects the next character with equal probability ($1/2N$), but the former must retain the possibility of actually ending the branch (as no word can be constructed from $\circ$en\=din\=g\=s$\bullet$). We denote the probability of ending a branch as $\alpha$. This $\alpha$ factor is removed uniformly from the transition probabilities away from a character that end words, hence $(1-\alpha)/2N$. Finally, the last row corresponds to the bottom node state, that cannot transition to anywhere.

The columns can similarly be partitioned into those four categories, where we define the following vectors
\begin{align}
\mathbf{v_T}& = \left[\begin{array}{c}
0 \\ 1/N \\ \vdots \\ 1/N \\ 0 \\ \vdots \\ 0 \\ 0
\end{array}\right] & 
\mathbf{v_{NW}}& = \left[\begin{array}{c}
0 \\ 1/2N \\ \vdots \\ 1/2N \\ 1/2N \\ \vdots \\ 1/2N \\ 0
\end{array}\right] & 
\mathbf{v_W}& = \left[\begin{array}{c}
0 \\ (1-\alpha)/2N \\ \vdots \\ (1-\alpha)/2N \\ (1-\alpha)/2N \\ \vdots \\ (1-\alpha)/2N \\ \alpha
\end{array}\right] & 
\mathbf{v_B}& = \left[\begin{array}{c}
0 \\ 0 \\ \vdots \\ 0 \\ 0 \\ \vdots \\ 0 \\ 1
\end{array}\right] & 
\end{align}
Clearly there are many linear dependencies between the columns (as there are $N$ each of $\mathbf{v_{NW}}$ and $\mathbf{v_W}$). An additional linear dependence that is of note is
\begin{equation}
\mathbf{v_W} = \alpha\mathbf{v_B} + (1-\alpha)\mathbf{v_{NW}}
\end{equation}

We proceed now to evaluate how each of these vectors is acted upon by $P$.
\begin{equation}
P_{ij}\mathbf{v_T} = \left[\begin{array}{c}
0\\N(1/N)(1/2N)\\\vdots\\N(1/N)(1/2N)\\0
\end{array}\right] = \left[\begin{array}{c}
0\\1/2N\\\vdots\\1/2N\\0
\end{array}\right]=\mathbf{v_{NW}}
\end{equation}
\begin{align}
P_{ij}\mathbf{v_{NW}} &= \left[\begin{array}{c}
0\\N(1/2N)^2 + N(1/2N)((1-\alpha)/2N)\\\vdots\\N(1/2N)^2 + N(1/2N)((1-\alpha)/2N)\\N(1/2N)\alpha
\end{array}\right] = \left[\begin{array}{c}
0\\(2-\alpha)/4N\\\vdots\\(2-\alpha)/4N\\\alpha/2
\end{array}\right]\\ 
&= \frac{2-\alpha}{2}\mathbf{v_{NW}} + \frac{\alpha}{2}\mathbf{v_B}
\end{align}
\begin{align}
P_{ij}\mathbf{v_{W}} &= \left[\begin{array}{c}
0\\N(1/2N)((1-\alpha)/2N) + N((1-\alpha)/2N)^2\\\vdots\\N(1/2N)((1-\alpha)/2N) + N((1-\alpha)/2N)^2\\N\alpha(1-\alpha)/2N + \alpha
\end{array}\right] = \left[\begin{array}{c}
0\\(1-\alpha)(2-\alpha)/4N\\\vdots\\(1-\alpha)(2-\alpha)/4N\\ \alpha(3-\alpha)/2
\end{array}\right]\\ 
&=\frac{2-\alpha}{2}\mathbf{v_W} + \frac{\alpha}{2}\mathbf{v_B}\label{eq:Pvw}
\end{align}
\begin{equation}\label{eq:Pvb}
P_{ij}\mathbf{v_B} = \left[\begin{array}{c}
0\\\vdots\\0\\1
\end{array}\right] = \mathbf{v_B}
\end{equation}
Note that $\mathbf{v_T}$ transitions into the ``not word'' state, which is what we wanted. Next, observe that the bottom node state transitions only to itself, and is therefore our first eigenvector, and has an eigenvalue of one.


\begin{align}
\mathbf{v_B}\cdot\mathbf{v_B} &= 1\\
\mathbf{v_T}\cdot\mathbf{v_T} &= 1/N\\
\mathbf{v_{NW}}\cdot\mathbf{v_{NW}} &= 1/2N\\
\mathbf{v_W}\cdot\mathbf{v_W} &= (1 - 2\alpha + (1+2N)\alpha^2)/2N\\
\mathbf{v_B}\cdot\mathbf{v_T} &= 0\\
\mathbf{v_B}\cdot\mathbf{v_{NW}} &= 0\\
\mathbf{v_B}\cdot\mathbf{v_W} &= \alpha\\
\mathbf{v_T}\cdot\mathbf{v_{NW}} &= 1/2N\\
\mathbf{v_T}\cdot\mathbf{v_W} &= (1-\alpha)/2N\\
\mathbf{v_{NW}}\cdot\mathbf{v_W} &= (1-\alpha)/2N\\
\end{align}

\begin{equation}
\mathbf{u_1} = \mathbf{v_B}
\end{equation}

\begin{equation}
\mathbf{u_2} = \mathbf{v_W} - \frac{\mathbf{v_B}\cdot\mathbf{v_W}}{\mathbf{v_B}\cdot\mathbf{v_B}}\mathbf{v_B}=\mathbf{v_W} - \alpha\mathbf{v_B}
\end{equation}
\begin{align}
P_{ij}\mathbf{u_2} &= P\left(\mathbf{v_W} -\alpha\mathbf{v_B}\right) = P\mathbf{v_W} -\alpha P\mathbf{v_B}\\
\intertext{Substituting from eqs \ref{eq:Pvw} and \ref{eq:Pvb}}
&= \frac{2-\alpha}{2} \mathbf{v_W} + \frac{\alpha}{2}\mathbf{v_B} - \alpha\mathbf{v_B}\\
&=\frac{2-\alpha}{2} \mathbf{v_W}-\frac{\alpha}{2}\mathbf{v_B}
\end{align}

Try
\begin{equation}
\mathbf{v_2} = \mathbf{v_B} - \mathbf{v_{NW}} = \left[\begin{array}{c}
0 \\ -1/2N \\ \vdots \\ -1/2N \\ -1/2N \\ \vdots \\ -1/2N \\ 1
\end{array}\right]
\end{equation}

\begin{align}
P_{ij}\mathbf{v_2} &= \left[\begin{array}{c}
0\\
N(-1/2N)(1/2N) + N(-1/2N)((1-\alpha)/2N)\\
\vdots\\
N(-1/2N)(1/2N) + N(-1/2N)((1-\alpha)/2N) \\
N(-1/2N)\alpha + 1
\end{array}\right] \\
&=\left[\begin{array}{c}
0\\
((2-\alpha)/2)(-1/2N)\\
\vdots\\
((2-\alpha)/2)(-1/2N) \\
(2-\alpha)/2
\end{array}\right]= \frac{2-\alpha}{2}\left[\begin{array}{c}
0\\-1/2N\\\vdots\\-1/2N\\1
\end{array}\right]=\frac{2-\alpha}{2}\mathbf{v_2}
\end{align}

\begin{equation}
\mathbf{v_2}\cdot\mathbf{v_2} = 2N\frac{1}{4N^2} + 1 = \frac{1 + 2N}{2N}
\end{equation}

Alternatively,
\begin{align}
P\mathbf{v_2} &= P(\mathbf{v_B} - \mathbf{v_{NW}}) = \mathbf{v_B} - (\frac{2-\alpha}{2}\mathbf{v_{NW}} + \frac{\alpha}{2}\mathbf{v_B})\\
&=\frac{2-\alpha}{2}(\mathbf{v_B} - \mathbf{v_{NW}}) = \frac{2-\alpha}{2}\mathbf{v_2}
\end{align}




\end{document}

















